{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI on VertexAI Reasoning Engine\n",
    "\n",
    "|||\n",
    "|----------|-------------|\n",
    "| Author(s)   | Christos Aniftos |\n",
    "| Reviewer(s) | |\n",
    "| Last updated | 2024 08 28  |\n",
    "| |   |\n",
    "\n",
    "This demo uses the default crewai project skeleton template to allow the use of Gemini model.\n",
    "At the time of the demo creation we used crewai version 0.51.0 and therefore some of the changes we mentioned might be outdate in future versions.\n",
    "As we already include a template in this repository **you will not need to create a new project**. \n",
    "\n",
    "However if you want to know more about starting a new CrewAI project from template look here: [Starting Your CrewAI Project](https://docs.crewai.com/getting-started/Start-a-New-CrewAI-Project-Template-Method/) . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "\n",
    "CrewAI is using poetry to manage dependencies. Lets install poetry so we can use it as well for our CrewAI project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets install poetry dependencies. \n",
    "\n",
    "In this repo we already added langchain-google-vertexai using `poetry add langchain-google-vertexai`. \\\n",
    "This allow the use of VertexAI model through the langchain model api.\n",
    "\n",
    "Below we explain how we change the llm used by crew agents to gemini-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry lock\n",
    "!poetry install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see some of the alternations we made to the original template in order to use Gemini as the llm that agents are using. \n",
    "Firstly we need to create a function in /src/crewai_gcp/crew.py that defines the llm. The function needs the llm annotation\n",
    "\n",
    "Here you can see the added in `./src/crewai_gcp/crew.py`  an llm function that returns a VertexAI model object from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -n -A 3  '@llm' ./src/crewai_gcp/crew.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally for every agent  configuration file in `./src/crewai_gcp/config/agents.yaml` we define the llm to be gemini_llm (same name as the function name we added in crew.py).\n",
    "\n",
    "By defing the llm, crewai knows to use the defined llm for each agent. If desired you can use different llm functions for different agents. For example an agent performing more complicated tasks might benefit from gemini-pro whereas agents with simpler operations might be performant with the lightweight gemini-flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -n -C 2 'llm:' ./src/crewai_gcp/config/agents.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our crew demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run crewai_gcp project locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run crewai_gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preapare CrewAI interface for Reasoning Engine\n",
    "\n",
    "To be able to run CrewAI on Reasoning Engine we need to create a class that defines an `__init__`, `setup` and `query` functions and crew_ai_app.py.\\\n",
    "Below you can see what we included in crew_ai_app.py.\\\n",
    "\n",
    "##### Some remarks:\n",
    "\n",
    "**Line 19**: We define what happens when Reasoning engine Starts. Here we initialise Traceloop.\\\n",
    "Traceloop provides an easy interface to trace our agent executions. We use `CloudTraceSpanExported` to export traces on Google Cloud Trace\n",
    "\n",
    "**Line 27**:  `@workflow(name=\"CrewAI_Trace\")` allows us to group all trace span together under CrewAI_Trace workflow. This name can change to whatever you want to name your trace grouping.\n",
    "\n",
    "**Line 29**:  `CrewaiGcpCrew().crew().kickoff(inputs={\"topic\": question})` runs the CrewAI for a given question. The response should be returned as __str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n ./crew_ai_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"sa-org-project\"\n",
    "location = \"us-central1\"\n",
    "staging_bucket = \"gs://sa-org-project-reasoning-engine\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crew_ai_app import CrewAIApp\n",
    "\n",
    "app = CrewAIApp(project=project_id, location=location)\n",
    "app.set_up()\n",
    "response_c = app.query(\"AI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy on reasoning engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import reasoning_engines\n",
    "\n",
    "vertexai.init(project=project_id, location=location, staging_bucket=staging_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reasoning_engine_list = reasoning_engines.ReasoningEngine.list()\n",
    "print(reasoning_engine_list)\n",
    "for re in reasoning_engine_list:\n",
    "    re.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a remote app with reasoning engine.\n",
    "# This may take 1-2 minutes to finish.\n",
    "from crew_ai_app import CrewAIApp\n",
    "\n",
    "reasoning_engine = reasoning_engines.ReasoningEngine.create(\n",
    "    CrewAIApp(project=project_id, location=location),\n",
    "    display_name=\"Demo Addition App\",\n",
    "    description=\"A simple demo addition app\",\n",
    "    requirements=[\n",
    "        'cloudpickle==3',\n",
    "        'crewai==0.51.0', \n",
    "        'langchain-google-vertexai==1.0.7',\n",
    "        \n",
    "        'traceloop-sdk==0.26.5',\n",
    "        'opentelemetry-exporter-gcp-trace==1.6.0'],\n",
    "    extra_packages=['./src','./crew_ai_app.py'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = reasoning_engine.query(question=\"Henry VIII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations:\n",
    "This section describes the limitations we encounter as of August 20th 2024\n",
    "\n",
    "### Memory: \n",
    "By default the [memory system of CrewAI](https://docs.crewai.com/core-concepts/Memory/?h=memory) is dissabled and deploying CrewAI without memory will work as intented.\\\n",
    "Enabling memory at the moment is not fully supported using reasoning engine because CrewAI uses local directory\\\n",
    "for memory data storage.\n",
    "When reasoning engine uses one isntaces this will work as intented but the logic will break\\\n",
    "with auto-scaling as new isntances will not share the same local files.\n",
    "\n",
    "This is not a problem with reasoning engine but a challenge with local storage systems when your deployment benefits\\\n",
    "from auto-scaling. To resolve this issue there is a need for external storage support.\\\n",
    "You can find more details on this [feature request](https://github.com/crewAIInc/crewAI/issues/1218) we submitted to the CrewAI team.\n",
    "\n",
    "### Vertex AI Embeddings \n",
    "CrewAI depends on [embedchain](https://github.com/mem0ai/mem0/tree/main/embedchain) library for generating embeddings.\n",
    "embedchain uses an old langchain import for VertexAIEmbeddings\\\n",
    "which is depricated and [fails pydantic field validation](https://github.com/crewAIInc/crewAI/issues/1213).\n",
    "\n",
    "A [PR was raised](https://github.com/mem0ai/mem0/pull/1717) in order to update to the newest  langchain VertexAIEmbeddings import that solves the issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "1) Check if crewai support streaming results to be used as api streaming (fastapi or flask)\n",
    "1) Check if parallel requests are supported in reasoning engine\n",
    "1) Benchmark local speed vs on reasoning engine\n",
    "1) Return other generated artifacts such us files as part of the api resonse\n",
    "1) Implement Human in the loop and how that can be achieved when deployed on reasoning engine.\\\n",
    "What happens when multiple users are using the app? how do we use sessions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
